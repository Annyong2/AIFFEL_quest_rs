{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a65b014",
   "metadata": {},
   "source": [
    "---   \n",
    "# ✅Prep A. Import Library & Dataset   \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ba77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04fddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 코어 수 (os): 4\n",
      "CPU 코어 수 (multiprocessing): 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import multiprocessing\n",
    "\n",
    "# os 모듈 수\n",
    "cpu_count_os = os.cpu_count()\n",
    "print(f\"CPU 코어 수 (os): {cpu_count_os}\")\n",
    "\n",
    "# multiprocessing 모듈 수\n",
    "cpu_count_mp = multiprocessing.cpu_count()\n",
    "print(f\"CPU 코어 수 (multiprocessing): {cpu_count_mp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52783c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_182/3927593637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from pandas.compat import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from pandas.compat.numpy import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnp_array_datetime64_compat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# numpy versioning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.util._decorators import (  # noqa\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[1;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# 데이터 처리 및 환경 설정\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy, copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import loralib as lora\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm  # 폰트 설정\n",
    "\n",
    "# from pykospacing import Spacing\n",
    "from hanspell import spell_checker\n",
    "\n",
    "# Supervised Fine-Tuning\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0824e",
   "metadata": {},
   "source": [
    "#### PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b59f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'skt/kogpt2-base-v2'\n",
    "model_save_path_1 = '/aiffel/KoChatGPT/output_1_SFT' \n",
    "model_save_path_2 = 'aiffel/KoChatGPT/output_2_RM'\n",
    "model_save_path_3 = 'aiffel/KoChatGPT/output_3_PPO'\n",
    "\n",
    "data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "data_path_2_RM  = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "data_path_3_PPO = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "\n",
    "output_dir = '/aiffel/KoChatGPT/test'\n",
    "logging_dir_1 = '/aiffel/KoChatGPT/logs/SFT_logs'\n",
    "logging_dir_2 = '/aiffel/KoChatGPT/logs/RM_logs'\n",
    "logging_dir_3 = '/aiffel/KoChatGPT/logs/PPO_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea554d",
   "metadata": {},
   "source": [
    "#### Setting Model , Tokenizer, Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990785f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 허깅페이스의 transformers모델과 토크나이저 정하기\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79bf4b",
   "metadata": {},
   "source": [
    "---   \n",
    "# ✅Prep B. EDA & Preprocessing   \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50554437",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1613b7",
   "metadata": {},
   "source": [
    "#### EDA : 데이터셋별 길이 확인 (max_length정하기 위하여 봄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "data_paths = {\n",
    "    \"SFT\": '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl',\n",
    "    \"RM\": '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl',\n",
    "    \"PPO\": '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이를 계산하고 저장할 딕셔너리\n",
    "lengths = {\"SFT\": [], \"RM\": [], \"PPO\": []}\n",
    "\n",
    "# 데이터 길이 계산 함수\n",
    "def calculate_lengths(data_path, data_type):\n",
    "    with open(data_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        list_data_dict = json.load(json_file)\n",
    "\n",
    "    for data in list_data_dict:\n",
    "        if data_type == \"SFT\":\n",
    "            # SFT는 prompt와 completion의 길이 합을 계산\n",
    "            text = data['prompt'] + \" \" + data['completion']\n",
    "        elif data_type == \"RM\":\n",
    "            # RM은 prompt와 completion들 중 하나의 합을 계산\n",
    "            text = data['prompt'] + \" \" + data['completion_0']\n",
    "        elif data_type == \"PPO\":\n",
    "            # PPO는 prompt만 사용\n",
    "            text = data['prompt']\n",
    "        \n",
    "        tokenized_text = tokenizer(text, return_tensors='pt')\n",
    "        lengths[data_type].append(tokenized_text['input_ids'].shape[1])  # 토큰 길이 추가\n",
    "\n",
    "# 80%를 포함하는 토큰 길이 계산 함수 \n",
    "def calculate_80_percentile_length(data, text_keys, tokenizer):\n",
    "    \"\"\"\n",
    "    전체 데이터의 80%를 포함하는 토큰 길이를 계산하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        data (list): 데이터 리스트. 각 항목은 딕셔너리 형태여야 하며, text_keys를 포함해야 함.\n",
    "        text_keys (list): 텍스트가 포함된 키 리스트 (예: ['completion_0', 'completion_1', 'completion_2'] 등)\n",
    "        tokenizer (transformers.AutoTokenizer): 텍스트 토크나이저 객체\n",
    "\n",
    "    Returns:\n",
    "        int: 전체 데이터의 80%를 포함하는 토큰 길이\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        for key in text_keys:\n",
    "            if key in item:  # 키가 존재하는 경우에만 처리\n",
    "                text = item[key]\n",
    "                tokenized_text = tokenizer(text, return_tensors='pt')\n",
    "                lengths.append(tokenized_text['input_ids'].shape[1])  # 토큰 길이 저장\n",
    "\n",
    "    percentile_80_length = int(np.percentile(lengths, 80))\n",
    "    return percentile_80_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebbd98",
   "metadata": {},
   "source": [
    "#### 데이터셋별 길이보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋의 길이 계산\n",
    "for data_type, data_path in data_paths.items():\n",
    "    calculate_lengths(data_path, data_type)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (data_type, length_data) in enumerate(lengths.items()):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.hist(length_data, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{data_type} Dataset Length Distribution')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 순차적으로 로드, 처리, 메모리 해제\n",
    "for data_type, data_path in data_paths.items():\n",
    "    with open(data_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        list_data_dict = json.load(json_file)\n",
    "\n",
    "    # 데이터셋별 key 설정\n",
    "    if data_type == \"SFT\":\n",
    "        text_keys = ['completion']\n",
    "    elif data_type == \"RM\":\n",
    "        text_keys = ['completion_0', 'completion_1', 'completion_2']\n",
    "    elif data_type == \"PPO\":\n",
    "        text_keys = ['prompt']\n",
    "    \n",
    "    # 80%를 포함하는 토큰 길이 계산 및 출력\n",
    "    percentile_80_length = calculate_80_percentile_length(list_data_dict, text_keys, tokenizer)\n",
    "    print(f\"{data_type} 데이터셋의 80%를 포함하는 Token Length: {percentile_80_length}\")\n",
    "    print(list_data_dict[50],\"\\n\",list_data_dict[80],\"\\n\",list_data_dict[97],\"\\n\",list_data_dict[270],\"\\n\",list_data_dict[410])\n",
    "    print()\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del list_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths = {\n",
    "    \"SFT\": 150,\n",
    "    \"RM\": 100,\n",
    "    \"PPO\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ace91",
   "metadata": {},
   "source": [
    "## Preprocessing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc0c9a",
   "metadata": {},
   "source": [
    "**<공통 전처리>**   \n",
    "[] 맞춤법 및 띄어쓰기 교정   \n",
    "[] 불필요한 문자 제거   \n",
    "[] 토큰 길이(Token Length) 조정    \n",
    "\n",
    "**<데이터셋별 특이 전처리>**   \n",
    "[] Reward Model : 다국어 문장 필터링    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50745ec",
   "metadata": {},
   "source": [
    "#### preprocess_data 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통1) 맞춤법 및 띄어쓰기 교정 함수\n",
    "def correct_text(text):\n",
    "    checked = spell_checker.check(text)\n",
    "    return checked.checked  # 교정된 텍스트 반환\n",
    "\n",
    "# 공통2) 불필요한 문자 제거\n",
    "def remove_unwanted_chars(text):\n",
    "    return re.sub(r\"[^\\w\\s가-힣.,!?]\", \"\", text)\n",
    "\n",
    "# 특이1) 다국어 필터링 함수 (한글과 영어 이외의 문자 제거)\n",
    "def filter_non_kr_en(text):\n",
    "    return re.sub(r\"[^가-힣a-zA-Z0-9\\s.,!?]\", \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "# 전처리 함수 (공통1,2,3 + 특이)\n",
    "def preprocess_data(data, data_type):\n",
    "    max_length = max_lengths[data_type]\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for item in data:\n",
    "        # 데이터셋에 따라 텍스트 설정\n",
    "        if data_type == \"SFT\":\n",
    "            text = item['prompt'] + \" \" + item['completion']\n",
    "            texts = [text]\n",
    "        elif data_type == \"RM\":\n",
    "            # 모든 completion 항목을 개별적으로 전처리\n",
    "            texts = [\n",
    "                item['prompt'] + \" \" + item['completion_0'],\n",
    "                item['prompt'] + \" \" + item['completion_1'],\n",
    "                item['prompt'] + \" \" + item['completion_2']\n",
    "            ]\n",
    "        elif data_type == \"PPO\":\n",
    "            text = item['prompt']\n",
    "            texts = [text]\n",
    "\n",
    "        # 각 텍스트에 대해 전처리 수행\n",
    "        for text in texts:\n",
    "            # 공통 전처리: 불필요한 문자 제거, 맞춤법 및 띄어쓰기 교정\n",
    "            text = remove_unwanted_chars(text)\n",
    "            text = correct_text(text)\n",
    "            \n",
    "            # Reward Model의 다국어 문장 필터링 (RM 데이터셋에만 적용)\n",
    "            if data_type == \"RM\":\n",
    "                text = filter_non_kr_en(text)\n",
    "            \n",
    "            # 토큰화 및 토큰 길이 조정\n",
    "            tokenized_text = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "            preprocessed_data.append(tokenized_text)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 각 데이터셋에 대해 전처리 수행\n",
    "for data_type, data_path in data_paths.items():\n",
    "    with open(data_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        list_data_dict = json.load(json_file)\n",
    "    \n",
    "    preprocessed_data = preprocess_data(list_data_dict, data_type)\n",
    "    print(f\"{data_type} 데이터셋의 전처리 완료, 샘플 수: {len(preprocessed_data)}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e1ae9",
   "metadata": {},
   "source": [
    "---   \n",
    "# ✅Train 1. Supervised Fine-Tuning      \n",
    "---\n",
    "(instruction dataset: kogpt-2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec28206",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1 = data_paths[\"SFT\"]\n",
    "\n",
    "with open(data_path_1, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict_1 = json.load(json_file)\n",
    "\n",
    "preprocessed_data_1 = preprocess_data(list_data_dict_1, data_type)\n",
    "print(f\"{data_type} 데이터셋의 전처리 완료, 샘플 수: {len(preprocessed_data_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 데이터셋 클래스 정의\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, preprocessed_data: list, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        self.input_ids = [item[\"input_ids\"].squeeze() for item in preprocessed_data]\n",
    "        self.labels = copy.deepcopy(self.input_ids)\n",
    "        for label in self.labels:\n",
    "            label[:len(label)] = -100\n",
    "\n",
    "        logging.warning(\"Loading data done!!: %d\" % (len(self.labels)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e642c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = SFT_dataset(preprocessed_data_1, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir, \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    "    logging_dir=logging_dir_1,\n",
    "    logging_steps=5,    \n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n",
    "model.save_pretrained(model_save_path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c8ebb",
   "metadata": {},
   "source": [
    "---   \n",
    "# ✅Train 2. Reward Model   \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Model\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_2 = data_paths[\"RM\"]\n",
    "\n",
    "with open(data_path_2, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict_2 = json.load(json_file)\n",
    "\n",
    "preprocessed_data_2 = preprocess_data(list_data_dict_2, data_type)\n",
    "print(f\"{data_type} 데이터셋의 전처리 완료, 샘플 수: {len(preprocessed_data_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Dataset 정의\n",
    "train_data = preprocessed_data_2[:int(len(preprocessed_data_2)*0.8)]\n",
    "eval_data = preprocessed_data_2[int(len(preprocessed_data_2)*0.8):]\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Model 정의 및 학습\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "value_head = nn.Linear(model.config.n_embd, 1)\n",
    "reward_model = RewardModel(model, value_head).to(device)\n",
    "trainer = RewardModelTrainer(\n",
    "    model=reward_model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=Adam(reward_model.parameters(), lr=5e-5),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    batch_size=4,\n",
    "    max_epochs=3,\n",
    "    logging_dir=logging_dir_2,\n",
    "    logging_steps=5\n",
    ")\n",
    "trainer.fit()\n",
    "reward_model.save_pretrained(model_save_path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14818133",
   "metadata": {},
   "source": [
    "## 비교 : SFT 모델 vs. RM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# 평가 함수 정의\n",
    "rouge = Rouge()\n",
    "\n",
    "def evaluate_model_bleu_rouge(predictions, references):\n",
    "    # BLEU 평가\n",
    "    bleu_scores = [sentence_bleu([ref], pred) for pred, ref in zip(predictions, references)]\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    # ROUGE 평가\n",
    "    rouge_scores = rouge.get_scores(predictions, references, avg=True)\n",
    "\n",
    "    return avg_bleu_score, rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 모델 예측 준비\n",
    "sft_predictions = []\n",
    "sft_references = []\n",
    "\n",
    "# SFT 모델 예측 및 참조 문장 준비\n",
    "for item in list_data_dict_1[:100]:  # 예시로 상위 100개의 샘플 사용\n",
    "    prompt = item['prompt']\n",
    "    reference = item['completion']\n",
    "    sft_references.append(reference)\n",
    "\n",
    "    # SFT 모델 예측 생성\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=150).input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_length=150, pad_token_id=tokenizer.pad_token_id)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    sft_predictions.append(prediction)\n",
    "\n",
    "# SFT 모델 BLEU 및 ROUGE 평가\n",
    "sft_avg_bleu_score, sft_rouge_scores = evaluate_model_bleu_rouge(sft_predictions, sft_references)\n",
    "print(\"SFT 모델 BLEU 점수:\", sft_avg_bleu_score)\n",
    "print(\"SFT 모델 ROUGE 점수:\", sft_rouge_scores)\n",
    "\n",
    "# RM 모델 예측 준비\n",
    "rm_predictions = []\n",
    "rm_references = []\n",
    "\n",
    "# RM 모델 예측 및 참조 문장 준비\n",
    "for item in list_data_dict_2[:100]:  # RM 데이터에서 상위 100개 샘플 사용\n",
    "    prompt = item['prompt']\n",
    "    reference = item['completion_0']  # 첫 번째 completion을 참조 문장으로 사용\n",
    "    rm_references.append(reference)\n",
    "\n",
    "    # RM 모델 예측 생성 (예: reward_model 사용)\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=100).input_ids.to(device)\n",
    "    output = reward_model.model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    rm_predictions.append(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a5fcb",
   "metadata": {},
   "source": [
    "#### RM 모델 BLEU 및 ROUGE 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc136b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 모델 BLEU 및 ROUGE 평가\n",
    "rm_avg_bleu_score, rm_rouge_scores = evaluate_model_bleu_rouge(rm_predictions, rm_references)\n",
    "print(\"RM 모델 BLEU 점수:\", rm_avg_bleu_score)\n",
    "print(\"RM 모델 ROUGE 점수:\", rm_rouge_scores)\n",
    "\n",
    "# SFT와 RM 모델 결과 비교\n",
    "print(\"\\nSFT 모델과 RM 모델의 BLEU 및 ROUGE 점수 비교:\")\n",
    "print(f\"SFT 모델 BLEU 점수: {sft_avg_bleu_score}, RM 모델 BLEU 점수: {rm_avg_bleu_score}\")\n",
    "print(f\"SFT 모델 ROUGE 점수: {sft_rouge_scores}, RM 모델 ROUGE 점수: {rm_rouge_scores}\")\n",
    "\n",
    "# 예측 및 참조 문장 출력 (정성적 평가)\n",
    "print(\"\\nSFT 모델과 RM 모델의 예측 및 참조 문장 비교 (상위 5개):\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n예시 {i + 1}:\")\n",
    "    print(f\"Prompt: {list_data_dict_1[i]['prompt']}\")\n",
    "    print(f\"SFT 모델 예측: {sft_predictions[i]}\")\n",
    "    print(f\"RM 모델 예측: {rm_predictions[i]}\")\n",
    "    print(f\"참조 문장 (정답): {sft_references[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe195b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6bf0a2",
   "metadata": {},
   "source": [
    "---   \n",
    "# ✅Train 3. Proximal Policy Optization   \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79512313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO를 위한 모델 초기화\n",
    "actor = GPTActor(pretrained=model_save_path_1, lora_rank=0).to(device)\n",
    "critic = GPTCritic(pretrained=model_save_path_2, lora_rank=0).to(device)\n",
    "initial_model = deepcopy(actor)\n",
    "reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 정의\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "# PPOTrainer 정의 및 학습\n",
    "ppo_trainer = PPOTrainer(\n",
    "    strategy=NaiveStrategy(),\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    reward_model=reward_model,\n",
    "    initial_model=initial_model,\n",
    "    actor_optim=actor_optim,\n",
    "    critic_optim=critic_optim,\n",
    "    max_epochs=3,\n",
    "    train_batch_size=8,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    logging_dir=logging_dir_3,\n",
    "    logging_steps=5\n",
    ")\n",
    "ppo_trainer.fit([data['prompt'] for data in list_data_dict_2], num_episodes=10, max_timesteps=3, update_timesteps=3)\n",
    "actor.save_pretrained(model_save_path_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
